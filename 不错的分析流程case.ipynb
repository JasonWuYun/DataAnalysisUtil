{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE, SelectKBest, chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import (BaggingClassifier,\n",
    "                              ExtraTreesClassifier,\n",
    "                              GradientBoostingClassifier,\n",
    "                              VotingClassifier,\n",
    "                              RandomForestClassifier,\n",
    "                              AdaBoostClassifier)\n",
    "###################################数据处################################\n",
    "######################################\n",
    "# 数据预处理\n",
    "######################################\n",
    "pd.set_option('display.max_columns', None)\n",
    "data = pd.read_csv('./data/data.csv')\n",
    "data.set_index('shot_id', inplace=True)\n",
    "data[\"action_type\"] = data[\"action_type\"].astype('object')\n",
    "data[\"combined_shot_type\"] = data[\"combined_shot_type\"].astype('category')\n",
    "data[\"game_event_id\"] = data[\"game_event_id\"].astype('category')\n",
    "data[\"game_id\"] = data[\"game_id\"].astype('category')\n",
    "data[\"period\"] = data[\"period\"].astype('object')\n",
    "data[\"playoffs\"] = data[\"playoffs\"].astype('category')\n",
    "data[\"season\"] = data[\"season\"].astype('category')\n",
    "data[\"shot_made_flag\"] = data[\"shot_made_flag\"].astype('category')\n",
    "data[\"shot_type\"] = data[\"shot_type\"].astype('category')\n",
    "data[\"team_id\"] = data[\"team_id\"].astype('category')\n",
    "unknown_mask = data['shot_made_flag'].isnull()\n",
    "data_cl = data.copy() # create a copy of data frame\n",
    "target = data_cl['shot_made_flag'].copy()\n",
    "# Remove some columns\n",
    "data_cl.drop('team_id', axis=1, inplace=True) # Always one number\n",
    "data_cl.drop('lat', axis=1, inplace=True) # Correlated with loc_x\n",
    "data_cl.drop('lon', axis=1, inplace=True) # Correlated with loc_y\n",
    "data_cl.drop('game_id', axis=1, inplace=True) # Independent\n",
    "data_cl.drop('game_event_id', axis=1, inplace=True) # Independent\n",
    "data_cl.drop('team_name', axis=1, inplace=True) # Always LA Lakers\n",
    "data_cl.drop('shot_made_flag', axis=1, inplace=True)\n",
    "data_cl['seconds_from_period_end'] = 60 * data_cl['minutes_remaining'] + data_cl['seconds_remaining']\n",
    "data_cl['last_5_sec_in_period'] = data_cl['seconds_from_period_end'] < 5\n",
    "data_cl.drop('minutes_remaining', axis=1, inplace=True)\n",
    "data_cl.drop('seconds_remaining', axis=1, inplace=True)\n",
    "data_cl.drop('seconds_from_period_end', axis=1, inplace=True)\n",
    "## Matchup - (away/home)\n",
    "data_cl['home_play'] = data_cl['matchup'].str.contains('vs').astype('int')\n",
    "data_cl.drop('matchup', axis=1, inplace=True)\n",
    "# Game date\n",
    "data_cl['game_date'] = pd.to_datetime(data_cl['game_date'])\n",
    "data_cl['game_year'] = data_cl['game_date'].dt.year\n",
    "data_cl['game_month'] = data_cl['game_date'].dt.month\n",
    "data_cl.drop('game_date', axis=1, inplace=True)\n",
    "# Loc_x, and loc_y binning\n",
    "data_cl['loc_x'] = pd.cut(data_cl['loc_x'], 25)\n",
    "data_cl['loc_y'] = pd.cut(data_cl['loc_y'], 25)\n",
    "# Replace 20 least common action types with value 'Other'\n",
    "rare_action_types = data_cl['action_type'].value_counts().sort_values().index.values[:20]\n",
    "data_cl.loc[data_cl['action_type'].isin(rare_action_types), 'action_type'] = 'Other'\n",
    "categorial_cols = [\n",
    "    'action_type', 'combined_shot_type', 'period', 'season', 'shot_type',\n",
    "    'shot_zone_area', 'shot_zone_basic', 'shot_zone_range', 'game_year',\n",
    "    'game_month', 'opponent', 'loc_x', 'loc_y']\n",
    "for cc in categorial_cols:\n",
    "    dummies = pd.get_dummies(data_cl[cc])\n",
    "    dummies = dummies.add_prefix(\"{}#\".format(cc))\n",
    "    data_cl.drop(cc, axis=1, inplace=True)\n",
    "    data_cl = data_cl.join(dummies)\n",
    "#  异常点检测方法\n",
    "def detect_outliers(series, whis=1.5):\n",
    "    q75, q25 = np.percentile(series, [75 ,25])\n",
    "    iqr = q75 - q25\n",
    "    return ~((series - series.median()).abs() <= (whis * iqr))\n",
    "# Separate dataset for validation\n",
    "data_submit = data_cl[unknown_mask]\n",
    "# 训练数据\n",
    "X = data_cl[~unknown_mask]\n",
    "Y = target[~unknown_mask]\n",
    "################################### Feature Selection###################\n",
    "#################################\n",
    "#RandomForestClassifier 来选择特征\n",
    "###############################\n",
    "threshold = 0.90\n",
    "vt = VarianceThreshold().fit(X)\n",
    "feat_var_threshold = data_cl.columns[vt.variances_ > threshold * (1-threshold)]\n",
    "feat_var_threshold\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, Y)\n",
    "feature_imp = pd.DataFrame(model.feature_importances_, index=X.columns, columns=[\"importance\"])\n",
    "feat_imp_20 = feature_imp.sort_values(\"importance\", ascending=False).head(20).index\n",
    "#################################\n",
    "# Univariate feature selection\n",
    "#################################\n",
    "X_minmax = MinMaxScaler(feature_range=(0,1)).fit_transform(X)\n",
    "X_scored = SelectKBest(score_func=chi2, k='all').fit(X_minmax, Y)\n",
    "feature_scoring = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'score': X_scored.scores_\n",
    "    })\n",
    "feat_scored_20 = feature_scoring.sort_values('score', ascending=False).head(20)['feature'].values\n",
    "feat_scored_20\n",
    "#################################\n",
    "# Recursive Feature Elimination\n",
    "#################################\n",
    "rfe = RFE(LogisticRegression(), 20)\n",
    "rfe.fit(X, Y)\n",
    "feature_rfe_scoring = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'score': rfe.ranking_\n",
    "    })\n",
    "feat_rfe_20 = feature_rfe_scoring[feature_rfe_scoring['score'] == 1]['feature'].values\n",
    "feat_rfe_20\n",
    "###############################\n",
    "# 合并所有特征选择方法的结果\n",
    "################################\n",
    "features = np.hstack([\n",
    "        feat_var_threshold,\n",
    "        feat_imp_20,\n",
    "        feat_scored_20,\n",
    "        feat_rfe_20\n",
    "    ])\n",
    "features = np.unique(features)\n",
    "print('Final features set:\\n')\n",
    "for f in features:\n",
    "    print(\"\\t-{}\".format(f))\n",
    "################################\n",
    "# clearn data\n",
    "###############################\n",
    "data_cl = data_cl.ix[:, features]\n",
    "data_submit = data_submit.ix[:, features]\n",
    "X = X.ix[:, features]\n",
    "print('Clean dataset shape: {}'.format(data_cl.shape))\n",
    "print('Subbmitable dataset shape: {}'.format(data_submit.shape))\n",
    "print('Train features shape: {}'.format(X.shape))\n",
    "print('Target label shape: {}'. format(Y.shape))\n",
    "#################################\n",
    "# PCA\n",
    "#################################\n",
    "components = 8\n",
    "pca = PCA(n_components=components).fit(X)\n",
    "pca_variance_explained_df = pd.DataFrame({\n",
    "    \"component\": np.arange(1, components+1),\n",
    "    \"variance_explained\": pca.explained_variance_ratio_\n",
    "    })\n",
    "ax = sns.barplot(x='component', y='variance_explained', data=pca_variance_explained_df)\n",
    "ax.set_title(\"PCA - Variance explained\")\n",
    "plt.show()\n",
    "###################################\n",
    "# 评估函数\n",
    "###################################\n",
    "seed = 7\n",
    "processors=1\n",
    "num_folds=3\n",
    "num_instances=len(X)\n",
    "scoring='log_loss'\n",
    "kfold = KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "#################################模型选则##############################\n",
    "#################################\n",
    "# 常用模型\n",
    "#################################\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('K-NN', KNeighborsClassifier(n_neighbors=5)))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "#models.append(('SVC', SVC(probability=True)))\n",
    "# Evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    cv_results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring, n_jobs=processors)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print(\"{0}: ({1:.3f}) +/- ({2:.3f})\".format(name, cv_results.mean(), cv_results.std()))\n",
    "##################################\n",
    "# Bootstrap Aggregation\n",
    "###################################\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring, n_jobs=processors)\n",
    "print(\"({0:.3f}) +/- ({1:.3f})\".format(results.mean(), results.std()))\n",
    "#####################################\n",
    "# Random Forest\n",
    "#####################################\n",
    "num_trees = 100\n",
    "num_features = 10\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=num_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring, n_jobs=processors)\n",
    "print(\"({0:.3f}) +/- ({1:.3f})\".format(results.mean(), results.std()))\n",
    "#####################################\n",
    "# extra tree\n",
    "#######################################\n",
    "num_trees = 100\n",
    "num_features = 10\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=num_features)\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring, n_jobs=processors)\n",
    "print(\"({0:.3f}) +/- ({1:.3f})\".format(results.mean(), results.std()))\n",
    "#######################################\n",
    "# AdaBoost\n",
    "######################################\n",
    "model = AdaBoostClassifier(n_estimators=100, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring, n_jobs=processors)\n",
    "print(\"({0:.3f}) +/- ({1:.3f})\".format(results.mean(), results.std()))\n",
    "############################################\n",
    "# Stochastic Gradient Boosting\n",
    "#############################################\n",
    "model = GradientBoostingClassifier(n_estimators=100, random_state=seed)\n",
    "results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring, n_jobs=processors)\n",
    "print(\"({0:.3f}) +/- ({1:.3f})\".format(results.mean(), results.std()))\n",
    "############################参数寻找################################\n",
    "#####################################\n",
    "# Logistic 参数寻找\n",
    "######################################\n",
    "lr_grid = GridSearchCV(\n",
    "    estimator = LogisticRegression(random_state=seed),\n",
    "    param_grid = {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.001, 0.01, 1, 10, 100, 1000]\n",
    "    },\n",
    "    cv = kfold,\n",
    "    scoring = scoring,\n",
    "    n_jobs = processors)\n",
    "lr_grid.fit(X, Y)\n",
    "print(lr_grid.best_score_)\n",
    "print(lr_grid.best_params_)\n",
    "#########################################\n",
    "# LinearDiscriminant\n",
    "########################################\n",
    "lda_grid = GridSearchCV(\n",
    "    estimator = LinearDiscriminantAnalysis(),\n",
    "    param_grid = {\n",
    "        'solver': ['lsqr'],\n",
    "        'shrinkage': [0, 0.25, 0.5, 0.75, 1],\n",
    "        'n_components': [None, 2, 5, 10]\n",
    "    },\n",
    "    cv = kfold,\n",
    "    scoring = scoring,\n",
    "    n_jobs = processors)\n",
    "lda_grid.fit(X, Y)\n",
    "print(lda_grid.best_score_)\n",
    "print(lda_grid.best_params_)\n",
    "#######################################\n",
    "# KNN\n",
    "############################################\n",
    "knn_grid = GridSearchCV(\n",
    "    estimator = Pipeline([\n",
    "        ('min_max_scaler', MinMaxScaler()),\n",
    "        ('knn', KNeighborsClassifier())\n",
    "    ]),\n",
    "    param_grid = {\n",
    "        'knn__n_neighbors': [25],\n",
    "        'knn__algorithm': ['ball_tree'],\n",
    "        'knn__leaf_size': [2, 3, 4],\n",
    "        'knn__p': [1]\n",
    "    },\n",
    "    cv = kfold,\n",
    "    scoring = scoring,\n",
    "    n_jobs = processors)\n",
    "knn_grid.fit(X, Y)\n",
    "print(knn_grid.best_score_)\n",
    "print(knn_grid.best_params_)\n",
    "###############################################\n",
    "# 寻找随机森林参数\n",
    "##############################################\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator = RandomForestClassifier(warm_start=True, random_state=seed),\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_features': [18, 20],\n",
    "        'max_depth': [8, 10],\n",
    "        'bootstrap': [True]\n",
    "    },\n",
    "    cv = kfold,\n",
    "    scoring = scoring,\n",
    "    n_jobs = processors)\n",
    "rf_grid.fit(X, Y)\n",
    "print(rf_grid.best_score_)\n",
    "print(rf_grid.best_params_)\n",
    "############################################\n",
    "# AdaBoost 参数寻找\n",
    "##############################################\n",
    "ada_grid = GridSearchCV(\n",
    "    estimator = AdaBoostClassifier(random_state=seed),\n",
    "    param_grid = {\n",
    "        'algorithm': ['SAMME', 'SAMME.R'],\n",
    "        'n_estimators': [10, 25, 50],\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1]\n",
    "    },\n",
    "    cv = kfold,\n",
    "    scoring = scoring,\n",
    "    n_jobs = processors)\n",
    "ada_grid.fit(X, Y)\n",
    "print(ada_grid.best_score_)\n",
    "print(ada_grid.best_params_)\n",
    "#################################################\n",
    "# GradientBoosting  \n",
    "#################################################\n",
    "gbm_grid = GridSearchCV(\n",
    "    estimator = GradientBoostingClassifier(warm_start=True, random_state=seed),\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [2, 3, 4],\n",
    "        'max_features': [10, 15, 20],\n",
    "        'learning_rate': [1e-1, 1]\n",
    "    },\n",
    "    cv = kfold,\n",
    "    scoring = scoring,\n",
    "    n_jobs = processors)\n",
    "gbm_grid.fit(X, Y)\n",
    "print(gbm_grid.best_score_)\n",
    "print(gbm_grid.best_params_)\n",
    "#################################################\n",
    "# 组合上面选择的模型\n",
    "#################################################\n",
    "estimators = []\n",
    "estimators.append(('lr', LogisticRegression(penalty='l2', C=1)))\n",
    "estimators.append(('gbm', GradientBoostingClassifier(n_estimators=200, max_depth=3, learning_rate=0.1, max_features=15, warm_start=True, random_state=seed)))\n",
    "estimators.append(('rf', RandomForestClassifier(bootstrap=True, max_depth=8, n_estimators=200, max_features=20, criterion='entropy', random_state=seed)))\n",
    "estimators.append(('ada', AdaBoostClassifier(algorithm='SAMME.R', learning_rate=1e-2, n_estimators=10, random_state=seed)))\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators, voting='soft', weights=[2,3,3,1])\n",
    "results = cross_val_score(ensemble, X, Y, cv=kfold, scoring=scoring,n_jobs=processors)\n",
    "print(\"({0:.3f}) +/- ({1:.3f})\".format(results.mean(), results.std()))\n",
    "###############################################\n",
    "# 预测\n",
    "###############################################\n",
    "model = ensemble\n",
    "model.fit(X, Y)\n",
    "preds = model.predict_proba(data_submit)\n",
    "submission = pd.DataFrame()\n",
    "submission[\"shot_id\"] = data_submit.index\n",
    "submission[\"shot_made_flag\"]= preds[:,0]\n",
    "submission.to_csv(\"sub.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
